{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\" id=\"heading\">Sentiment Analysis of Reddit Data using Reddit API</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( 🤗 the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the session, you will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- know how to work with APIs\n",
    "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
    "- understand what a `pipeline` object is in HuggingFace\n",
    "- perform sentiment analysis using `pipeline`\n",
    "- run a python script in command line and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of each task, commit* the work into the repository you created before the assignment\n",
    "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your repository you created before the assignment\n",
    "- Submit the link to the notebook in Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task I: Instantiate a Reddit API Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please ensure you've ran all the cells in the `imports.ipynb`, located [here](https://github.com/FourthBrain/MLE-8/blob/main/assignments/week-3-analyze-sentiment-subreddit/imports.ipynb), to make sure you have all the required packages for today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2. Create a new app on Reddit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a Reddit account if you don't have one, log into your account.\n",
    "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
    "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect uri** ( The redirect URI is where the user is sent after they've granted OAuth access to your application (more info [here](https://github.com/reddit-archive/reddit/wiki/OAuth2)) For our purpose, you can enter some random url, e.g., www.google.com; as shown below.\n",
    "\n",
    "\n",
    "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
    "- Jot down `client_id` (left upper corner) and `client_secret` \n",
    "\n",
    "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
    "    \n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "- Create `secrets_reddit.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"client_id\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"secret_id\"\n",
    "    REDDIT_API_USER_AGENT = \"any string except bot; ex. My User Agent\"\n",
    "    ```\n",
    "- Add `secrets_reddit.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Instantiate a `Reddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import secrets_reddit\n",
    "\n",
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets_reddit.REDDIT_API_CLIENT_ID,\n",
    "    client_secret=secrets_reddit.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent=secrets_reddit.REDDIT_API_USER_AGENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0x7fd4aa718190>\n"
     ]
    }
   ],
   "source": [
    "print(reddit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Instantiate a `subreddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected output you will see ar from `r/machinelearning` unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"doge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the display name of the subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doge\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    machinelearning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about its title, is it different from the display name? \n",
    "\n",
    "Answer: Yes, but it is only different in this case when it comes to lower versus upper case. Some titles and display names can be the same. For example the subreddit funny has the display_name of 'funny' and a title of 'funny'. In general when you look at a subreddit page, the r/{name} is the display name and the name directly above it is the title name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    Machine Learning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Print out the description of the subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[](http://www.reddit.com/message/compose/?to=CelestialWalrus&subject=Sidebar+ad+on+rdoge&message=Put+your+300x250+ad+link+in+here&?sidebarad)\n",
      "\n",
      "[](http://www.reddit.com/r/doge?spyingdoge)\n",
      "\n",
      "[Free sidebar ad in /r/doge](http://www.reddit.com/message/compose/?to=CelestialWalrus&subject=Sidebar+ad+on+rdoge&message=Put+your+300x250+ad+link+in+here)\n",
      "\n",
      "**Rules:**\n",
      "\n",
      "[Additional explainations for these rules can be found on our Rules wiki page.](https://old.reddit.com/r/doge/wiki/rules)\n",
      "\n",
      "*hover for details*\n",
      "\n",
      "| | |\n",
      "|-|-|\n",
      "|1. No posts that are not related to Doge|This is subreddit for mainly Kabosu-related (the \"original\" doge) content, but other animals are allowed (and photoshops with Doge).|\n",
      "|1a. No Forced / Ironic / Surreal Doge Posts|This rule has been expanded to cover 'forced' Doge posts that feature the original 'Doge' image, but have been modified in such a way that does not relate to the Doge meme. For clarification, please see the [Ironic Doge Meme KnowYourMeme](https://knowyourmeme.com/memes/ironic-doge-memes) page for examples of what **is not** allowed.|\n",
      "|2. Dogecoin/cryptocurrency posts are not allowed here.|Do not post anything related to the cryptocurrency called Dogecoin here. Please, do not ask questions or make posts about Dogecoin/other cryptocurrencies. Go to /r/dogecoin for dogecoin, /r/bitcoin for bitcoin, etc... ~~Go to /r/dogetipbot or /r/changetip to learn how to give tips.~~ [DogeTipBot has ended. For more information click here](https://www.reddit.com/r/dogecoin/comments/7l8hxv/dogetipbot_final_update/).|\n",
      "|3. No personal information|Do not doxx other people, and do not link to anything hosted on Facebook servers, as you can easily track the content to original account holder. Any personal information should be censored.|\n",
      "|4. No gore or porn|Posting gore or porn of any kind in this subreddit will result in a permanent ban.|\n",
      "|5. Please report post breaking subreddit or site wide rules|Leave a detailed reason when reporting, if possible.|\n",
      "|6. No personal attacks, bigotry, or otherwise derogatory or offensive content|All personal attacks, bigotry (including racism, homophobia, transphobia, etc.) and all other attacks or slurs are strictly forbidden, and can result in a permanent ban.|\n",
      "|7. Spam is not allowed.|Our rule on Spam includes all links to online merchants/shops. We cannot vet all merchant links to verify their validity, and thus are not allowed in posts and comments. If you'd like to share merchant/shop links, please do so using direct messages. **Use extreme caution when visiting and purchasing from any shops.**|\n",
      "|8. Such wow, many fun, very reddit|Sorry for the serious rules :<|\n",
      "\n",
      "**Shibegenerators/other neat doge related sites:**\n",
      "\n",
      "* http://www.webrender.net/shibe/\n",
      "\n",
      "**Related subreddits:**\n",
      "\n",
      "* /r/SuperShibe - more popular doge subreddit\n",
      "* /r/dogecoin - such cryptocurrency, wow!\n",
      "* /r/abcqwerty123 - the /b/ of reddit\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>\n",
    "\n",
    "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
    "    --------\n",
    "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
    "    --------\n",
    "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
    "    --------\n",
    "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
    "    --------\n",
    "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task II: Parse comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. Top Posts of All Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# try run this line, what do you see? press q once you are done\n",
    "# Answer: The ? before a method gives you the manual page for the method you are trying to use.\n",
    "#         The manual page is a reference to the documention for the method called after the '?', in this case\n",
    "#         it brings up the manual page for the method subreddit.top\n",
    "?subreddit.top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE PHONES HOME with Elon\n",
      "3160\n",
      "mrpkjg\n",
      "https://i.redd.it/66cp6qmtwet61.jpg\n",
      "Wow. So wolf. Much howl. Very moon.\n",
      "1930\n",
      "1s5rf8\n",
      "http://i.imgur.com/klb812s.jpg\n",
      "Lost Doge\n",
      "1758\n",
      "1no2y0\n",
      "http://i.imgur.com/4tz2eNt.jpg\n",
      "Such angle many neck wow\n",
      "1310\n",
      "1r7e49\n",
      "http://i.imgur.com/e4CiCs7.gif\n",
      "The Firefox icon has never looked better\n",
      "1281\n",
      "1t32gx\n",
      "https://people.mozilla.org/~smartell/meme/such-logo.gif\n",
      "Much knife. Such Stab. Wow.\n",
      "1236\n",
      "1r3shx\n",
      "http://i.imgur.com/z4MPTZb.png\n",
      "Wow such PlayStation\n",
      "1225\n",
      "1pzbnb\n",
      "http://imgur.com/CAU3pJa\n",
      "I will never get over how perfect this is.\n",
      "1228\n",
      "25e2fv\n",
      "http://i.imgur.com/uMwpKP0.gif\n",
      "Such wink! Such sleep!\n",
      "1131\n",
      "mkm6sg\n",
      "https://i.imgur.com/wn3HXHG.png\n",
      "doge on a calculator\n",
      "1038\n",
      "1n7od6\n",
      "http://i.imgur.com/QaqHdPg.jpg\n"
     ]
    }
   ],
   "source": [
    "for submission in subreddit.top(limit=10, time_filter=\"all\"):\n",
    "    print(submission.title)\n",
    "    # Output: the submission's title\n",
    "    print(submission.score)\n",
    "    # Output: the submission's score\n",
    "    print(submission.id)\n",
    "    # Output: the submission's ID\n",
    "    print(submission.url)\n",
    "    # Output: the URL the submission points to or the submission's URL if it's a self post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details> <summary>Expected output:</summary>\n",
    "\n",
    "    [Project] From books to presentations in 10s with AR + ML\n",
    "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
    "    [R] First Order Motion Model applied to animate paintings\n",
    "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
    "    [D] This AI reveals how much time politicians stare at their phone at work\n",
    "    [D] Types of Machine Learning Papers\n",
    "    [D] The machine learning community has a toxicity problem\n",
    "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
    "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
    "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. Top 10 Posts of This Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Elon make Doge The Currency of Twitter?\n",
      "8\n",
      "yjmflb\n",
      "https://i.redd.it/7lbngmn0uex91.jpg\n"
     ]
    }
   ],
   "source": [
    "for submission in subreddit.top(limit=10, time_filter=\"week\"):\n",
    "    print(submission.title)\n",
    "    # Output: the submission's title\n",
    "    print(submission.score)\n",
    "    # Output: the submission's score\n",
    "    print(submission.id)\n",
    "    # Output: the submission's ID\n",
    "    print(submission.url)\n",
    "    # Output: the URL the submission points to or the submission's URL if it's a self post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details><summary>Expected output:</summary>\n",
    "\n",
    "    [N] Ian Goodfellow, Apple’s director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said “I believe strongly that more flexibility would have been the best policy for my team.” He was likely the company’s most cited ML expert.\n",
    "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
    "    [P] I’ve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple’s version of MobileNet, and more directly on your phone's camera roll.\n",
    "    [R] Meta is releasing a 175B parameter language model\n",
    "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
    "    [P] T-SNE to view and order your Spotify tracks\n",
    "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
    "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
    "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
    "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "Check out what other attributes the `praw.models.Submission` class has in the [docs](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html). \n",
    "\n",
    "1. After having a chance to look through the docs, is there any other information that you might want to extract? How might this additional data help you?\n",
    "\n",
    "*Answer: Yes, I found the number of comments for a submission, the upvote ration (percentage of upvotes from all votes on the submission), and the submissions's score helpful as a start. This could help with sentiment analysis because the higher the percentage of upvotes, the higher the score, and the higher the number of comments generally could inform a sentiment analysis model. In particular, the upvote_ratio is a direct measure of user happiness with the submission. These three information fields are also beneficial because they are simply numbers and are easier to work with than say pictures or text. \n",
    "\n",
    "*Additionally, you can pull the comments attached to a submission and analyze each in turn. These could be either text or image comments. An initial manual read over the comments gives us an idea of the sentiment of a submission and how well it matches the original three metrics already mentioned above (num_comments, upvote_ratio, submission.comments). Of note, the comments can be of various output types but will be returned in one list in the format of a 'CommentForest' object. See code below for what that might look like for all time top 10.\n",
    "\n",
    "Write a sample piece of code below extracting three additional pieces of information from the submission below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "0.99\n",
      "3158\n",
      "Doge to all the way to 1 dollar\n",
      "I’ve made over 8,000 in 35 days! I’m loving it!\n",
      "Robinhood shut that shit down just like they did GME!!! Can't buy or sell right now!\n",
      "i'll allow it\n",
      "What’s a good amount to invest to doge right now ? I don’t want to buy too much or too little\n",
      "[deleted]\n",
      "Is this ever going to stop?\n",
      "We broke the doge! RH says outage\n",
      "I’d wait right now I think .33 was the peak and we’re gonna see the correction now. Some scary things are one man holds 20 percent of the supply, 50 percent is owned by 5 people. And because it’s not capped. To get doge to 500 roughly 1/100th of but coin not even. It would take more then the worlds entire gdp\n",
      "Yo buy the dips bois\n",
      "im predicting Dogie to Dump... a turd on the ground 🤣🐶🔥💯\n",
      "Listen to I'm A Alien by LiricoTheKiDD on #SoundCloud\n",
      "https://soundcloud.app.goo.gl/5CLoM\n",
      "Lots of blessings let’s get it 🙏 god bless 🙏\n",
      "Risking it all for $$$ memes 🚀\n",
      "What is everyones doge strategy if you bought late  at like .15 ?\n",
      "`Im holding my doge ! ^^^^^ going up`\n",
      "Wtf just happened, I just made 5k haha holy shit\n",
      "We need the Apes to help push this\n",
      "wow  \n",
      "photoshop 10/10  \n",
      "idea: 10/10  \n",
      "humor: 10/10\n",
      "Go doge or go home\n",
      "So realistic everyone gonna buy more and 💎 this or are we crashing ?.\n",
      "https://www.reddit.com/r/CryptoCurrency/comments/ms6p31/everyone_needs_to_hear_this_from_charles/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n",
      "Hodl, in you wanna get rich, just hodl\n",
      "To the moon 🤩☝️\n",
      "Doge to the muuuuuuuuthaaafoookin moonnn\n",
      "Great artistic rendering 👊\n",
      "That is one of my favorite DOGE pics🥇💰💻🥇\n",
      "Elon broke our dreams\n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌   \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐   \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐   \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐   \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌   \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌   \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐   \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌   \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌   \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐   \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌   \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐   \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌   \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐   \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌   \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀   \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀   \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀\n",
      "WoW\n",
      "DOGE DOGE DOGE DOGE TOOOO THE MOOOOOONNNNN ALLL ABOOOAAARRRRDDDD THE DOGGEEETRAIIINN\n",
      "PLEASE WAIT TO SELL DOGE WHEN IT GETS TO .50 OR HIGHER! STOP TAKING OUT AT 3,4,5 CENT GAINS!!!\n",
      "PLEASE UP VOTE SO I CAN GET KARMA TO POST PLEASE!!!!\n",
      "Hilarious.\n",
      "That's great lmfao\n",
      "Doge spelled backwards is eGOD! Count your blessings boys and girls! And feel free to coin that phrase!\n",
      "Am paying with Dogecoin 🚀\n",
      "I would move all DOGECOIN to SAFEMOON for the next ride to the moon 🌙🚀\n",
      "As Elon said. No Highs No Lows, Only Doge 🙌\n",
      "Doge stonk boys\n",
      "Jesus what is the new floor and time to buy more\n",
      "Created some artwork for this crazy event!\n",
      "\n",
      "https://rarible.com/token/0xd07dc4262bcdbf85190c01c996b4c06a461d2430:312472:0xe5e7fa9912033055092d2bd0d9ae78b3b784c48e\n",
      "Sell your car, the money on DOGE\n",
      " I  know an employee that works for Coinbase, he said that from what he is  hearing Doge is way over valued and a correction will ensue around   Saturday/Late Friday. I asked what price the coin would drop to and he   insisted on a price of $0.12.......So sell now and wait for the drop  guys.\n",
      "\n",
      "Now, of course anyone can  question anybody and whether or not you trust what he said is up to you.  But judging by this guys experience in crypto and the company he works  for, he knows his sh\\*t. Just thought id post in here and share the $0.02  I was given to you all. Good luck guys!\n",
      "\n",
      "If you want you can vote if you think Dogecoin will hit $100 in the future here: [https://www.dogeto100.com](https://www.dogeto100.com/)\n",
      "DOGE WILL HIT 1.00 we are literally half way there\n",
      "Why did we let it fall after Robinhood hood jammed it\n",
      "Aug 26 is international Doge day!!! Everyone best be buying on Aug 26 #dogecoin\n",
      "Pappaa!!!!\n",
      "Why are there no further updates? It's as if everything is frozen...\n",
      "Everyone needs to ask Elon to accept doge for teslas. If he doesn't it could fall and some people may be holding the bag that can't afford to. These are desperate times after all.\n",
      "Hit him up on Twitter\n",
      "This is painful\n",
      "Check out Dogira coin made from Doge Creators!\n",
      "Hi,\n",
      "\n",
      "If you believe, like me, in economic inclusion and the distribution of prosperity for all, then I invite you to sign up for GoodDollar, create your own basic income wallet and start collecting your daily digital income.\n",
      "\n",
      "Use my invite link and receive an extra 50G$ bonus\n",
      "\n",
      "The total amount is only 1.5 million, so he may rise to $1,000 each, and the way to receive it is free, and only needs to perform face recognition to prevent robots from landing. If you receive a total of 100, it may be worth $100,000 in 3 years\n",
      "\n",
      "[https://wallet.gooddollar.org?inviteCode=2TRG2mHe9H](https://wallet.gooddollar.org?inviteCode=2TRG2mHe9H)\n",
      "sell off or DIP with reason to buy? Hard to tell right now. I bought in at 0.05 so im good for a while but gonna dump soon if this doesnt pick up\n",
      "Doge to the moon\n",
      "The time for Big Short has come\n",
      "[IG](https://www.instagram.com/reel/CNxXLHanGd-/?igshid=zsrs6dd2ezbx)\n",
      "I made 11k.... but freaking doge dropped . Now only 6k 😭😭\n",
      "Doge will eventually take the #1 spot since bitcoin is overpriced the masses are now getting a second chance at a winner and Doge is breaking all the rules this will be the people’s currency “Kennedy firing room to all doge holders please closed and lock your visors...12, 11, be advised we are in a smooth count” main engine start......3,2,1, Gods speed everyone we are going past the moon\n",
      "SCREW ROBINHOOD! Webull now allows $doge coin trading! Here the limited beta invite on Twitter link! Will fill up fast! JOIN NOW \n",
      "\n",
      "https://twitter.com/twitermytweet/status/1383781739600830465?s=21\n",
      "Wassup with the pump & dump? Jesus Christ.\n",
      "Sooooooooo who sold\n",
      "Cant stand ppl that are still like “when are we going to moon?” Like it went from .07 to .47 wtf you mean where’s moon. That’s how you know ain’t nothing going to ever be good enough.\n",
      "💪🏽💪🏽💪🏽\n",
      "THE MOON go TO, doge must\n",
      "Doge on webull\n",
      "How high can we go Elon? 😎\n",
      "Is it too late to buy a lot at .30\n",
      "Fuckin hilarious\n",
      "Ride on\n",
      "1130p ET. People realize that Elon is on SNL on the 8th. Not the 3rd. Doge price corrects immediately\n",
      "I’m up 5k with 7000 coins I feel lucky.\n",
      "https://www.reddit.com/r/TuttiFruttiFi/comments/n5n2l7/this_is_too_the_moon_then_sun/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n",
      "https://www.reddit.com/r/TuttiFruttiFi/comments/n5n2l7/this_is_too_the_moon_then_sun/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n",
      "Make 50k in a moth\n",
      "🤣🤣🤣🤣🤣🤣🤣\n",
      "Is that tiesto\n",
      "I think it's just people selling off\n",
      "Not selling a single coin until it hits $200...\n",
      "To the Mooooon\n",
      "Man, a lot of you guys are gonna be in for a big surprise here soon....\n",
      "Earlier this week, the market capitalization of Dogecoin hit an all-time high (it’s around $79 billion, as of the time of this writing). The automaker Honda Motor Co. Ltd., meanwhile, has a market cap right now of a little more than $54 billion. Why are we bringing Honda into the picture? Well, it’s because Markus revealed in an interview with the finance news site Benzinga back in March that he’d sold off all of his holdings in the cryptocurrency in 2015 for about what it would cost to buy a used Honda Civic at the time — a little less than $10,000.     Is this going to be you too when you sell DOGE at $1???\n",
      " \n",
      "\n",
      "🤍 SAFEJIZZ $JIZZ 🤍\n",
      "\n",
      "🥞PANCAKESWAP JUST LAUNCHED, PUMPING HARD!!\n",
      "\n",
      "[https://exchange.pancakeswap.finance/#/swap?outputCurrency=0xa197BAFDcBC1ff388734e088BaE732110273e246&inputCurrency=BNB](https://exchange.pancakeswap.finance/#/swap?outputCurrency=0xa197BAFDcBC1ff388734e088BaE732110273e246&inputCurrency=BNB)\n",
      "\n",
      "LP is burned!\n",
      "\n",
      "✳️ Telegram: [https://t.me/safejizz](https://t.me/safejizz)\n",
      "\n",
      "⚡️GO GO GO\n",
      "I made a song called \"To The Moon\" lets make it go viral!\n",
      "https://open.spotify.com/track/1IWgaWKlJpKJAdP0FIJSD8?si=UhgCypEmTR6v6m6g7373-Q&utm_source=copy-link\n",
      "Who here wants doge to drop a bit to buy the dip ??\n",
      "Jesus Christ SNL is pathetic and disgusting to watch. It used to be funny.\n",
      "I love you Doge!\n",
      "Lets get him back home.\n",
      "Fuck off with your shitcoins\n",
      "Don’t you guys see he used Doge coin as a marketing tool for his show . Which was one of the cleverest strategies I’ve ever seen. Getting the masses involved by involving money the you would get rich. And a lot of people did make money no doubt including me. \n",
      "But this whole dodge coin and getting rich and then dumping the coins he bought to gauge the prices and dumping it right in the middle of the show 😂 look at the bigger picture. This man is a fucking genius have to give him that\n",
      "Doge needed cooling anyways, much rest then much wow\n",
      "Gay\n",
      "Pomp Shiba\n",
      "That was filmed around the block where I grew up.\n",
      "To the moon\n",
      "Elon phone doge.\n",
      "0TO BUY PANTHERCOIN OR CLAIM YOUR AIRDROP.\n",
      "\n",
      "1️⃣ Copy and paste the link on your DAPP browser in your trust wallet app\n",
      "\n",
      "2️⃣ Change network from ETH to Binance Smart Chain\n",
      "\n",
      "3️⃣ Scroll down, enter the value of PTC you wish to buy.\n",
      "Minimum buy 0.01 = 100,000,000\n",
      "Maximum buy 10 BNB = 100,000,000,000\n",
      "\n",
      "4️⃣ Ensure you have sufficient BnB smart chain for gas fee and payment for the coin\n",
      "\n",
      "\n",
      "Good luck and ask questions for further guidance\n",
      "\n",
      "\n",
      "https://panthercoin.net/airdrop/?ref=0xBBB5cF062025dD63C937d8CFe13dEffD580E1EF8\n",
      "Good time to buy\n",
      "What platform is the group using to buy crypto?\n",
      "I think the run is done !!!\n",
      ".29 what chart are you looking at\n",
      "Looks like home is 0.21\n",
      "😂\n",
      "I wish\n",
      "Coming back around!!\n",
      "As close doge will get to the moon, on bike\n",
      "😁👍🏼\n",
      "$NDL!!!\n",
      "Hand it up. Elon if anything is hurting doge than helping.\n",
      "Sold my car and put the money into doge . Who needs a car when you have a rocket\n",
      "Moon walk !\n",
      "Yea\n",
      "Gemini and crypto.com sells Doge, and s few more now\n",
      "OMG this is great. 🤣❤\n",
      "Guys look into baby doge..if you missed out on DOGE..less then 2 weeks new and already hit 100million market cap..over 50k holders still till this day..great telegram community with over 30k members and we fight whales! Buy the dip and hodl!! Please don’t join safebabydoge it’s a scam why are they saying “ babydoge killer “ when they stole the name and info... but we have big plans!!\n",
      "Screw Elon\n",
      "DogeElonMusk token on LOBSTR. Can be like Doge Token? Will go up like Doge?\n",
      "Buy🦍🦍🦍🦍🦍🦍🙌💎💎💎💎\n",
      "Cuttee\n",
      "We’re all joining force together  make sure this baby is not going left behind!!!instead going forward !!!\n",
      "Great… hide him in your bathroom until it hits $1 lol 😂😂😂\n",
      "Maybe drag LadyDoge with it. \n",
      "https://www.ladydoge.net/\n",
      "DOGE is the biggest waste of time and money\n",
      "Doge making a comeback to .05 here we go!!!\n",
      "He needs a mk19 to blow it up. 🚀🚀🌙\n",
      "🤣🤣🤣 Cyborg Bonnie reporting for duty. Elon Musk, I have a question, is that why you're not at risk for the virus because you are more advanced than than we are as far as cyborgs go?\n",
      "PLEASE!!!! STOP SELLING DOGE WHEN IT RISES 3,4,5 CENTS! LET IT GO UP! WHEN IT HITS .30+ THEN SELL OFF U HAVE TO, BUT TRY AND LET IT GET TO .50 OR HIGHER!!!\n",
      "It’s decentralized! That means they don’t know what, if any gains/sold crypto! I think! Correct me if I’m wrong please!\n",
      "To the moon 🌛\n",
      "Doge, hello!\n",
      "ET?\n",
      "Clearly an old post.  I dumped two hours before Elon’s antics on SNL and bought it back at like .19.  Thanks Mr Musk. I really don’t like you for this.\n",
      "Missed out on Doge? Missed out on Babydoge?  \n",
      "Dont miss out   \n",
      "u/RacingDoge!  \n",
      "NFT Market ✅  \n",
      "Game .Betting 🔥  \n",
      "6% in and out 👍  \n",
      "dApp coming 🤑  \n",
      "t.me/RacingDoge  \n",
      "The next Doge family 💎  \n",
      "\\#racingdoge  \n",
      "@PancakeSwap  \n",
      "@binance  \n",
      "@cz\\_binance  \n",
      "@Bsc100x\\_gems  \n",
      "@BscGemz  \n",
      "@CoinMarketCap  \n",
      "@elonmusk\n",
      "Doge will never make it to the moon if people don't stop selling the second they make just a couple cents a eth. Also if we just stop buying or just waiting for the dip to buy.\n",
      "It’s crazier then ever but hopefully that’s going to be a good thing eventually!\n",
      "Blow up BTT it’s still low\n",
      "28\n",
      "0.92\n",
      "1936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg im in tears right now.\n",
      "At first I was like..what is this? then I saw the face 10/10 thank you for making me laugh\n",
      "       wow \n",
      "\n",
      "                so subtle\n",
      "\n",
      "      adoge photoshop\n",
      "\n",
      "       \n",
      "Fucking Moon Moon. \n",
      "      wow\n",
      "                                              many wild\n",
      "\n",
      "                      so original doge\n",
      "\n",
      "                                                 much photogenic\n",
      "              very bff\n",
      "\n",
      "                                                              wow\n",
      "Such shop\n",
      "\n",
      "Much Blend\n",
      "\n",
      "Wow\n",
      "\n",
      "Very Impress\n",
      "          wow \n",
      "\n",
      "                                                    such front page\n",
      "\n",
      "                       very successful \n",
      "\n",
      "       \n",
      "such woofe\n",
      "wow\n",
      "can not contain\n",
      "very wild\n",
      "Wow\n",
      "It makes me uncomfortable. Not sure why...\n",
      "Goddamnit moon moon!\n",
      "              wow\n",
      "                    much side pains\n",
      "     very lol            \n",
      "                                    WOW\n",
      "          such stares\n",
      "Shibes are genetically closest to wolves of all doge breeds. now you know. Wow.\n",
      "Well done!\n",
      "Actually makes me think about how we've tried so hard to make dogs still look like puppies.  Look at how big the eyes are compared to the actual wolf!\n",
      "Very subtle. I didn't even see what you did until I took a second look.\n",
      "Much seamless. Very art.\n",
      "such wuff\n",
      "\n",
      "wow\n",
      "\n",
      "very growl\n",
      "\n",
      "woof\n",
      "epic shop skillz  \n",
      "much howling  \n",
      "wow\n",
      "http://www.reddit.com/r/SuperShibe/comments/1rxmha/such_nature_so_habitat/\n",
      "Well shit.\n",
      "34\n",
      "0.98\n",
      "1760\n",
      "Can I have the plain image to print out? \n",
      "Goldinating /u/Trogdor_T_Burninator\n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀ \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀ \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀  \n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌      \n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐      \n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐      \n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐      \n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌       \n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌      \n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐      \n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌      \n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌      \n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐      \n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌       \n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐      \n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌      \n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐      \n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌      \n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀      \n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀       \n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀\n",
      "This actually makes me more sad than anything.\n",
      "I saw one of these taped to a bulletin board at my school and cracked up\n",
      "\n",
      "(School as in university)\n",
      "found domino doge\n",
      "http://www.youtube.com/watch?v=_gUuPiONH8g\n",
      "I saw one of these beauties at my school, I will admit I laughed pretty hard.\n",
      "Doge is like, the fucking cutest dog I've ever seen.\n",
      "I don't get it.  I see these Doge things around and I can't understand how they are funny.  Could someone please explain so that I can share in the laughter?\n",
      "i printed like 50 copes and posted them all around school. that was a fun week!\n",
      " \n",
      "Lol, this picture is from my school\n",
      "My favorite part is that it's in Comic Sans for that extra giggle.\n",
      "Holy fucking shit. I just cried out loud with laughter man. \n",
      "I'm right here.\n",
      "wowowowoowowowowoow\n",
      "\n",
      "hahaha WORST doge evar!\n",
      "\n",
      "http://gifmovie.tumblr.com/post/69367648720/doge-gif\n",
      "Did someone steal your Doge?\n",
      "How are all the comments here from 7-8 years ago? What’s going on here?!\n",
      "This post is from 8 years ago! What the Doge is going on here?!\n",
      "░░░░░░░░░▄░░░░░░░░░░░░░░▄░░░░\n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌░░░\n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐░░░\n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐░░░\n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐░░░\n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌░░░\n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌░░\n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐░░\n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌░\n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌░\n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐░\n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌\n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐░\n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌░\n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐░░\n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌░░\n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀░░░\n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀░░░░░\n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀▀░░░░░░░░\n",
      "░░░░░░░░▌▒█░░░░░░░░░░░▄▀▒▌\n",
      "░░░░░░░░▌▒▒█░░░░░░░░▄▀▒▒▒▐\n",
      "░░░░░░░▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐\n",
      "░░░░░▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐\n",
      "░░░▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌\n",
      "░░▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌\n",
      "░░▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐\n",
      "░▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌\n",
      "░▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌\n",
      "▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐\n",
      "▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌\n",
      "▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐\n",
      "░▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌\n",
      "░▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐\n",
      "░░▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌\n",
      "░░░░▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀\n",
      "░░░░░░▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀\n",
      "░░░░░░░░░▒▒▒▒▒▒▒▒▒▒▀\n",
      "12\n",
      "0.96\n",
      "1306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Wrong\" is the best part. \n",
      "      wow\n",
      "               such repost\n",
      "    many karma \n",
      "              wow\n",
      "         deja shibe \n",
      "                      \n",
      "I laughed way too hard at this. \n",
      "agility, not 2 worry Haha\n",
      "http://www.youtube.com/watch?v=gvdf5n-zI14\n",
      "Cutest doge gif ever\n",
      "18\n",
      "0.91\n",
      "1274\n",
      "[deleted]\n",
      "If you did this, I fucking love you. If you didn't, I still fucking love you for showing it to me. \n",
      "     wow             very surprise\n",
      "\n",
      "             much unexpected\n",
      "\n",
      "                                              so browser\n",
      "           such firefox\n",
      "made a .ico out of it\n",
      "\n",
      "download it here\n",
      "http://www.megafileupload.com/en/file/480586/IconDogeFoxe-ico.html\n",
      "\n",
      "sample\n",
      "http://nsa33.casimages.com/img/2013/12/20/131220015938346075.png\n",
      "You forgot, \"many addon\"\n",
      "This is so excellently done.\n",
      "Excellent work. Improved my day. \n",
      "The wapapapapow really took it to the next level\n",
      "much scary\n",
      "\n",
      "edit: wow\n",
      "This almost makes me want to switch back to Firefox.\n",
      "Not sure if Mozilla marketing or fanmade\n",
      "I really want the gif to just loop between the ffox logo and doge enhancement.\n",
      "Could someone create a 128x128 logo for me and my mac? Pretty please with sugar on top (I have no photoshop, nor the skillz to pay said billz)...\n",
      "I upvoted the shit out of this\n",
      "\n",
      "17\n",
      "0.95\n",
      "1231\n",
      "As a 911 operator, I hope that one day I get a call like this \n",
      "wow\n",
      "so language\n",
      "very speak\n",
      "wow\n",
      "much understand\n",
      "Very hurt, wow, blood loss.\n",
      "Interesting username\n",
      "Well,  you're fucked then. \n",
      "*ow\n",
      "[Yesterday](http://www.reddit.com/r/doge/comments/1r3shx/much_knife_such_stab_wow/)\n",
      "hahahahahahaha\n",
      "What did I just read. I’m latterly dieting from laughter\n",
      "😿🙊😿\n",
      "14\n",
      "0.97\n",
      "1224\n",
      "wow \n",
      "many respect\n",
      "such playstation\n",
      "very buy\n",
      "wow\n",
      "\n",
      "such peasant\n",
      "Here's the link to the actual comment:\n",
      "https://www.facebook.com/PlayStation/posts/10152038711561803?comment_id=31899565&reply_comment_id=31899741&total_comments=87\n",
      "I love how everyone include Sony can't get enough doge so success\n",
      "wow such repost\n",
      "Such repost\n",
      "\n",
      "              Very scandalous\n",
      "Wow\n",
      "\n",
      "                                Many downvote\n",
      "25\n",
      "0.94\n",
      "1229\n",
      "when he pulls out the ball i just lose it, gets me every time\n",
      "It IS great to be a member of Doge!\n",
      "Wow\n",
      "What are these dogs personalities like? I kinda want one....\n",
      "if you watched closely, i think he just took a piss in the beginning. \n",
      "\n",
      "or maybe the camera shook a little?\n",
      "This is what made doge for me\n",
      "[Dig Doge](http://img1.wikia.nocookie.net/__cb20130410133438/smashbroslawlorigins/images/8/82/Dig_Dug.png)\n",
      "You're right! This is PERFECT! I love this.\n",
      "Such ball.\n",
      "So squint. Wow. Much racism. \n",
      "You'll probably get over it eventually.\n",
      "I can't be the only one who fucking hates these . \n",
      "126\n",
      "1.0\n",
      "1126\n",
      "Very shibe much cute wow!\n",
      "Soft doge, warm doge  \n",
      "Little ball of wow\n",
      "\n",
      "Happy doge, sleepy doge  \n",
      "Wow, wow, wow\n",
      "I wish I can wake up in the morning with this look and attitude on my face.\n",
      "Such adorbs!\n",
      "c00t\n",
      "I swear this looked like sushi in my notification 🤣\n",
      "Freaking adorable LMAO\n",
      "Apart from robbinhood where can I buy doge\n",
      "1 Doge = 1 Doge!\n",
      "Wow 😯\n",
      "Lucky human having an adorably happy doge like that 😍\n",
      "Hell yeah keep holding and buying more 🐶🐶🐶🐶\n",
      "Nice!\n",
      "10k Family blessing\n",
      "SCREW ROBINHOOD! Webull now allows $doge coin trading! Here the limited beta invite on Twitter link! Will fill up fast! JOIN NOW \n",
      "\n",
      "https://twitter.com/twitermytweet/status/1383781739600830465?s=21\n",
      "DL2TNwk8YPtD76iCMQXW2gBmTsQ9wX5TVP DOGE to the moon\n",
      "Awwww\n",
      "Much wow\n",
      "I made a song called \"To The Moon\" lets make it go viral!\n",
      "https://open.spotify.com/track/1IWgaWKlJpKJAdP0FIJSD8?si=UhgCypEmTR6v6m6g7373-Q&utm_source=copy-link\n",
      "Such cute!\n",
      "[doegcoin supremacy NOW!](https://youtu.be/QlVcXQZhynk)\n",
      "To the moon!🚀🚀🚀\n",
      "\t\n",
      "www.grayscale.co\n",
      "\n",
      "Big corporate is about to take your money, follow the money before it’s too late!\n",
      "9\n",
      "0.97\n",
      "1038\n",
      "wow                        much patience \n",
      "\n",
      "doge              wow\n",
      "\n",
      "          math impress\n",
      "So procrastinating\n",
      "The limit no exist. Such wow. \n",
      "wow such amaze \n",
      "very respect\n",
      "how did get\n",
      "\n",
      "hahaha WORST doge evar!\n",
      "http://gifmovie.tumblr.com/post/69367648720/doge-gif\n",
      "wow much calculator very graphs so acurate\n",
      "TI-84 Moar\n",
      "For anyone wondering, yes this is the original doge calculator meme. Nothing predates it. I am the lore\n"
     ]
    }
   ],
   "source": [
    "for submission in subreddit.top(limit=10, time_filter=\"all\"):\n",
    "    print(submission.num_comments)\n",
    "    # Output: the number of comments on a submission\n",
    "    print(submission.upvote_ratio)\n",
    "    # Output: the percentage of upvotes from all votes on the submission\n",
    "    print(submission.score)\n",
    "    # Output: the submission's number of upvotes\n",
    "    for sub in submission.comments:\n",
    "        print(sub.body)\n",
    "    # Output: the submissions comments\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "2. Is there any information available that might be a concern when it comes to Ethical Data? \n",
    "\n",
    "*Answer: When it comes to the subreddit object and what the api offers as attributes, there is some information that might be a concern to Ethical Data. Firstly, some data violates privacy of reddit users. Some examples include the attributes 'author','comments',and possibly 'poll_data' since it can contain voting information. The 'comments' field could have revealing information about the user or others through the use of links, private discussions held in a public discourse, and tagged users to name a few examples. Of note, the 'name' attribute appears to already be anonymized so this attribute is not a concern to privacy. In summary, the 'author', 'poll_data', and 'comments' attributes should either be excluded or anonymized and aggregated as much as possible.\n",
    "\n",
    "*In addition to privacy concerns, there is some potential bias in the data fields such as 'created_utc' and 'comments'. For the former, the 'created_utc' string tells us the exact time distribution of submissions. For example, the top 10 all time DOGE subreddit submissions are dominated by the years 2013 and 2014. This could skew the sentiment analysis to what was positive and negative back in 2013 and 2014 rather than the present day. Some effort should be made to assume all years are equally represented as much as possible. For the latter, the 'comments' are posted by real reddit users with real biases, especially in community of interests that a user joins if they generally already like the thread. In the case of r/doge, you can two very different communities already within the comments. One of them is related around crypto and the other around general doge memes. This even led to the r/doge managers banning dogecoin/cryptocurrency posts. Nevertheless, these comments persist in the history of this subreddit and should probably be removed. \n",
    "\n",
    "*Examples of the attributes 'author' and 'created_utc', as mentioned above' are shown in the code chunk below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thesaafdaaf\n",
      "2021-04-15 18:19:42\n",
      "Emperor_NOPEolean\n",
      "2013-12-05 08:53:21\n",
      "aguilar_s24\n",
      "2013-10-03 14:07:00\n",
      "SkyyLord\n",
      "2013-11-22 02:48:55\n",
      "tobiasahlin\n",
      "2013-12-17 07:32:02\n",
      "GoNavy_09\n",
      "2013-11-20 20:25:51\n",
      "None\n",
      "2013-11-05 17:11:39\n",
      "RankedQueue\n",
      "2014-05-12 16:49:03\n",
      "VerGuy\n",
      "2021-04-05 10:38:22\n",
      "None\n",
      "2013-09-26 19:50:32\n"
     ]
    }
   ],
   "source": [
    "import datetime #library to change UNIX time stamps from reddit to date formats for easy viewing\n",
    "\n",
    "for submission in subreddit.top(limit=10, time_filter=\"all\"):\n",
    "    print(submission.author)\n",
    "    # Output: the author of the submission\n",
    "    print(datetime.datetime.fromtimestamp(submission.created_utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. Comment Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "\n",
    "The purpose is \n",
    "1. to understand what the code is doing \n",
    "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later 😊) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.2 ms, sys: 9.63 ms, total: 78.8 ms\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time # prints wall time for entire cell, where wall time is how much time has passed on a clock\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Store comments in list 'top_comments'\n",
    "top_comments = []\n",
    "\n",
    "# Loop over the top 10 submissions for the chosen subreddit and grab all top level comments\n",
    "for submission in subreddit.top(limit=10):\n",
    "    # Loop over each comment in the specific submission (uses a 'CommentForest' object)\n",
    "    for top_level_comment in submission.comments:\n",
    "        # If the comment is a NOT a top level comment, then ignore it (avoids a potential error when you call comment.body)\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # If the comment is a top level comment, append it to our stored comments for the subreddit\n",
    "        top_comments.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. Inspect Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many comments did you extract from the last step? Examine a few comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255  comments extracted\n",
      "Example comments below:\n",
      "I’ve made over 8,000 in 35 days! I’m loving it!\n",
      "Robinhood shut that shit down just like they did GME!!! Can't buy or sell right now!\n",
      "i'll allow it\n",
      "Blow up BTT it’s still low\n",
      "*ow\n",
      "Much wow\n",
      "For anyone wondering, yes this is the original doge calculator meme. Nothing predates it. I am the lore\n"
     ]
    }
   ],
   "source": [
    "# Show the number of top level comments extracted from the subreddit top 10\n",
    "print(len(top_comments), \" comments extracted\")\n",
    "\n",
    "# Analyze some top level comments\n",
    "print(\"Example comments below:\")\n",
    "print(top_comments[1]) # comment from when crypto took over the r/doge subreddit\n",
    "print(top_comments[2]) # comment from when crypto took over the r/doge subreddit - notice the temperment change\n",
    "print(top_comments[3]) # comment from when crypto took over the r/doge subreddit\n",
    "print(top_comments[133]) # comment from when crypto took over the r/doge subreddit, last one before it shifts to older doge memes\n",
    "print(top_comments[200]) # unclear comment, shows that sometimes format characters are in the strings\n",
    "print(top_comments[240]) # Standard doge meme comment\n",
    "print(top_comments[254]) # Standard doge meme comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow',\n",
       " 'This is so excellently done.',\n",
       " 'Gemini and crypto.com sells Doge, and s few more now']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "[random.choice(top_comments) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
    "\n",
    "    ['Awesome visualisation',\n",
    "    'Similar to a stack or connected neurons.',\n",
    "    'Will this Turing pass the Turing Test?']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "3. After having a chance to review a few samples of 5 comments from the subreddit, what can you say about the data? \n",
    "\n",
    "HINT: Think about the \"cleanliness\" of the data, the content of the data, think about what you're trying to do - how does this data line up with your goal?\n",
    "\n",
    "*Answer: If we want to complete sentiment analysis on the subreddit r/doge data, there are few things of note about the data. First, a large chunk of the top level comments are from the dogecoin/cryptocurrency community briefly taking over the r/doge subreddit in 2021, specifically 133/255 top level comments. This might not be prudent to include in sentiment analysis and could be defined as noise that requires removal. Second, the comments are a mix of text, images, images made from text codes, tagged users, emoji, and specific links. Some of these have privacy concerns and will need to be anonymized. Additionally, some of these may not be useful if we are using a text only sentiment-analyzer since they are images. Third, some comments use special characters such as '*' and will require data cleaning before use. Fourth, many of the original r/doge comments are very old so some effort should be made to see if data outside 2013 and 2014 is available to keep the data as unbiased as possible.   \n",
    "\n",
    "*In summary, the r/doge comments will require cleaning, some top-level comments might need to be removed based on data type (image, text, emoji, etc) depending on what type of sentiment analysis is used, removal of data that is considered noise, some fields in the text comments will require anonymization, and finally the data might be biased to 2013-2104 so some effort to capture more data in extra years is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiate subreddit object for 'TSLA'\n",
    "subreddit_tsla = reddit.subreddit(\"TSLA\")\n",
    "\n",
    "# Grab Top level comments for top 10 topics from the last year\n",
    "# Store comments in list 'top_comments'\n",
    "top_comments_tsla = []\n",
    "\n",
    "# Loop over the top 10 submissions for the chosen subreddit and grab all top level comments\n",
    "for submission in subreddit_tsla.top(limit=10,time_filter=\"year\"):\n",
    "    # Loop over each comment in the specific submission (uses a 'CommentForest' object)\n",
    "    for top_level_comment in submission.comments:\n",
    "        # If the comment is a NOT a top level comment, then ignore it (avoids a potential error when you call comment.body)\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # If the comment is a top level comment, append it to our stored comments for the subreddit\n",
    "        top_comments_tsla.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments_tsla) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Holding 415 shares since 450 (presplit)\\nI'm gonna hold for another 5-10 years.\\n\\n\\nIf Tsla eventually overpasses Apple as the most valuable company ill become a Teslionaire!!\",\n",
       " 'I’m still just going to wait for a stock split',\n",
       " '75 Shares baby \\n\\n90% of my portfolio']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(top_comments_tsla) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
    "\n",
    "    ['I bought puts',\n",
    "    '100%',\n",
    "    'Yes. And I’m bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "4. Now that you've had a chance to review another subreddits comments, do you see any differences in the kinds of comments either subreddit has - and how might this relate to bias?\n",
    "\n",
    "*Answer: The r/TSLA subreddit's top level comments for the top 10 submissions over the last year are heavily biased towards stock related conversations. Additionally, there is a bullish trend to the comments surrounding stocks. These comments are not balanced in terms of sentiment and might very well show very little negative sentiment when passed through a sentiment analysis. Once can also expect to see a lot of neutral comments since there are a lot of numbers being stated in conjuction with stock prices. \n",
    "\n",
    "*When this is compared to the r/doge community, there are very different kinds of comments. In stark comparision to r/TSLA, the r/doge community outright outlaws stock/crypto/dogecoin chatter. The r/doge community also is very graphics and text-based image driven versus more of the text driven r/TSLA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task III: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Import `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "random.seed(1) # set seet for reproducibility of 'comment' below\n",
    "comment = random.choice(top_comments_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 from 500 days. Bought one more during 800 low last week. Wishing my tax return would come in before giga Austin opens up!'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Make Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "sentiment = sentiment_model(comment) # predict sentiment of comment using huggingFace sentiment analysis task\n",
    "print(type(sentiment)) # get type of output for variable 'sentiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the type of the output `sentiment`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "The type of output for sentiment is 'list'.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment: 10 from 500 days. Bought one more during 800 low last week. Wishing my tax return would come in before giga Austin opens up!\n",
      "Predicted Label is NEGATIVE and the score is 0.999\n"
     ]
    }
   ],
   "source": [
    "print(f'The comment: {comment}')\n",
    "print(f'Predicted Label is {sentiment[0][\"label\"]} and the score is {sentiment[0][\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example comment, the output is:\n",
    "\n",
    "    The comment: Bury Burry!!!!!\n",
    "    Predicted Label is NEGATIVE and the score is 0.989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🖥️❓ Model Question:\n",
    "\n",
    "1. What does the score represent?\n",
    "\n",
    "*Answer: This score represents the probability distribution of the two labels: POSITIVE and NEGATIVE. According to the paper on this model(https://arxiv.org/pdf/1910.01108v4.pdf) a standard softmax function is applied at inference (aka predictions). This means that the total value of all the scores must equal one. The label with the highest score is what the model chooses to classify the comment as, whether it be POSITIVE or NEGATIVE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Put All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the piece together, create a simple script that does \n",
    "\n",
    "- get the subreddit\n",
    "- get comments from the top posts for given subreddit\n",
    "- run sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%writefile top_tlsa_comment_sentiment.py\n",
    "\n",
    "import secrets\n",
    "import random\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.subreddit import Subreddit\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_subreddit(display_name:str) -> Subreddit:\n",
    "    \"\"\"Get subreddit object from display name\n",
    "\n",
    "    Args:\n",
    "        display_name (str): [description]\n",
    "\n",
    "    Returns:\n",
    "        Subreddit: [description]\n",
    "    \"\"\"\n",
    "    reddit = Reddit(\n",
    "        client_id=secrets.REDDIT_API_CLIENT_ID,        \n",
    "        client_secret=secrets.REDDIT_API_CLIENT_SECRET,\n",
    "        user_agent=secrets.REDDIT_API_USER_AGENT\n",
    "        )\n",
    "    \n",
    "    subreddit = # YOUR CODE HERE\n",
    "    return subreddit\n",
    "\n",
    "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
    "    \"\"\" Get comments from subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit (Subreddit): [description]\n",
    "        limit (int, optional): [description]. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of comments\n",
    "    \"\"\"\n",
    "    top_comments = []\n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            top_comments.append(top_level_comment.body)\n",
    "    return top_comments\n",
    "\n",
    "def run_sentiment_analysis(comment:str) -> Dict:\n",
    "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
    "    \n",
    "    Args:\n",
    "        comment (str): [description]\n",
    "        \n",
    "    Returns:\n",
    "        str: Sentiment analysis result\n",
    "    \"\"\"\n",
    "    sentiment_model = # YOUR CODE HERE\n",
    "    sentiment = sentiment_model(comment)\n",
    "    return sentiment[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    subreddit = # YOUR CODE HERE\n",
    "    comments = get_comments(subreddit)\n",
    "    comment = # YOUR CODE HERE\n",
    "    sentiment = run_sentiment_analysis(comment)\n",
    "    \n",
    "    print(f'The comment: {comment}')\n",
    "    print(f'Predicted Label is {sentiment[\"label\"]} and the score is {sentiment[\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python top_tlsa_comment_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Expected output:</summary>\n",
    "\n",
    "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "    The comment: When is DOGE flying\n",
    "    Predicted Label is POSITIVE and the score is 0.689\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "5. Is the subreddit active? About how many posts or threads per day? How could you find this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "6. Does there seem to be a large distribution of posters or a smaller concentration of posters who are very active? What kind of impact might this have on the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "c57794392b841cffd8686d5c4548e4e2ec78521f49300d60954d1380f1b4bd1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
