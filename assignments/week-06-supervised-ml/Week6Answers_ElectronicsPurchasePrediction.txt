====================Algorithm Understanding
Q: Is SVM (Support Vector Machine) a supervised or unsupervised learning algorithm?
A: SVM is a supervised machine learning model.

Q: Why is SVM such a powerful classification method?
A: SVM is a powerful classification method because it allows us to linearly 
   separate a space by converting the data to a higher dimension. It then can
   apply a linear seperator and then once  this seperator is cast down to the 
   lower dimension it can take on the shape of a non-linear decision boundary. 
   Additionally, SVM uses support vectors which enable the decision boundary 
   to enforce a set distance between 2 classes we are trying to classify. 
   This 'maximised margin' helps with over-fitting and generalization. These
   support vectors also make the decision function memory efficient, since it
   does not need to track the other data points. Lastly, it can be effective 
   when the number of features is greater than the number of samples.

Q: What are 3 disadvantages of SVMs?
A: First, it does not perform well when we have a large data set due to the 
   higher required training time. Second, it doesn't perform well when the data 
   set has more noise (target classes overlapping). Third, SVM does not provide
   probability estimates, but rather one must calculate it using five-fold 
   cross-validation. 

====================Interview Readiness
Q: What is the time complexity of SVM?
A: The time complexity for an SVM is O(n^2) for training time complexity. 
   The run-time complexity is O(k*d), where k is the number of support
   vectors and d is the dimensionality of the data. 

Q: What is it for Logistic Regression?
A: The time complexity for Logistic Regression is O(n*d) for training time
   complexity, where n is the number of training examples and d is the number 
   of dimensions of the data aka feature count. Space complexity is O(d).

====================Interview Readiness
Q: Explain feature importance for the Random Forest algorithm?
A: Feature importance is calculated as the decrease in node impurity
   (where node impurity is the measure of homogeneity of the labels at
   the node in question) weighted by the probability of reaching that node. 
   The node probability can be calculated by the number of samples that reach
   the node divided by the total number of samples. The higher the value the
   more important the feature. 

Q: When examining feature importance, what is Gini impurity or information gain?
A: Gini impurity is used to predict the likelihood that a randomly selected
   example would be incorrectly classified by a specific node. A Gini impurity of
   0 says that all elements belong to a single class. A Gini impurity of 1 says
   that all elements are randomly distributed over various classes. A Gini 
   impurity of 0.5 says that all elements are uniformly distributed across the
   classes. 

   Information gain is the process of selecting the best features to provide
   the most information about a class. It does this by attempting to reduce
   the entrophy from the root node to the leaf nodes. Specifically at a node, 
   the difference in entropy is measured before and after splitting and the 
   information gain is recorded like so: information gain = 1 - entropy, where
   entropy is the measure of a random variable's uncertainty or roughly speaking
   how much variance the data has. Essentially, Information gain is how much 
   entropy we removed. Aka higher informatiion gain == more entropy removed ==
   better quality/useful split at that node. 

====================Interview Readiness
Q: SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain
   the output of any machine learning model, what is it and how does it work?
A: SHAP is an approach to explain models via coalitional game theory, which tells
   us how to fairly distribute the "payout" among features for a prediction.

   It works by assigning each feature a contribution
   to a specific instance's prediction and then using each feature's contribution 
   to explain the difference between the average prediction and the specific 
   instance's prediction. The feature with the most contributions, or profit, 
   has more importance. Additionally, features can work together in 'coalitions'
   to contribute more.
   
   Taken all feature contributions together, this is called 
   the 'Shapley values' for a model. Of note, the shapley value is NOT the 
   the difference in prediction if we would remove that feature from the model. 
   To reiterate, Shapley values is the average contribution of a feature value 
   to the prediction in different coalition. 